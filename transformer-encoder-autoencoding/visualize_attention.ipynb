{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from encoder import TransformerEncoder\n",
    "from utils import build_vocab, encode_batch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Sample data\n",
    "sent = 'Transformers use [MASK] attention'\n",
    "target = 'Transformers use self attention'\n",
    "vocab = build_vocab([sent, target])\n",
    "max_len = len(sent.split())\n",
    "model = TransformerEncoder(len(vocab), 64, 4, 2, 128, max_len)\n",
    "src_ids = encode_batch([sent], vocab, max_len)\n",
    "src_tensor = torch.tensor(src_ids)\n",
    "out, attn = model(src_tensor)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(attn[0][0].detach().numpy(), annot=True, cmap='viridis')\n",
    "plt.title('Attention Weights (Head 0)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
